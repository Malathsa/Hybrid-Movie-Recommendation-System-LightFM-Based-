{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb22b8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/layan/anaconda3/envs/lightfm_env/lib/python3.10/site-packages/lightfm/_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ===== 1) Imports =====\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightfm.data import Dataset\n",
    "from lightfm import LightFM\n",
    "from lightfm.evaluation import precision_at_k, auc_score, recall_at_k\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import coo_matrix\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7517f4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings data: (100836, 4)\n",
      "Movies data: (9742, 3)\n",
      "Tags data: (3683, 4)\n"
     ]
    }
   ],
   "source": [
    "# ===== 2) Load Data =====\n",
    "# Load the MovieLens ratings file containing user‚Äìmovie interactions and ratings\n",
    "ratings = pd.read_csv(\"ratings.csv\")\n",
    "\n",
    "# Load the movies file containing movie metadata such as titles and genres\n",
    "movies = pd.read_csv(\"movies.csv\")\n",
    "\n",
    "# Load the tags file containing user-generated tags describing movie content\n",
    "tags = pd.read_csv(\"tags.csv\")\n",
    "\n",
    "# Display the dimensions of each dataset to verify successful loading\n",
    "# and understand the scale of the data\n",
    "print(f\"Ratings data: {ratings.shape}\")\n",
    "print(f\"Movies data: {movies.shape}\")\n",
    "print(f\"Tags data: {tags.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a435cf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä BIAS DETECTION - Major-Minor Ratio\n",
      "============================================================\n",
      "üé≠ Genre Counts (Top 10):\n",
      "Drama        4361\n",
      "Comedy       3756\n",
      "Thriller     1894\n",
      "Action       1828\n",
      "Romance      1596\n",
      "Adventure    1263\n",
      "Crime        1199\n",
      "Sci-Fi        980\n",
      "Horror        978\n",
      "Fantasy       779\n",
      "dtype: int64\n",
      "\n",
      "üìâ Genre Counts (Bottom 10):\n",
      "Film-Noir       87\n",
      "IMAX           158\n",
      "Western        167\n",
      "Musical        334\n",
      "War            382\n",
      "Documentary    440\n",
      "Mystery        573\n",
      "Animation      611\n",
      "Children       664\n",
      "Fantasy        779\n",
      "dtype: int64\n",
      "\n",
      "üü¶ Major‚ÄìMinor Ratio: 50.13\n",
      "Most common genre has: 4361 movies\n",
      "Least common genre has: 87 movies\n",
      "‚ö†Ô∏è Warning: Strong genre imbalance detected!\n"
     ]
    }
   ],
   "source": [
    "# ===== 3) Detect Bias =====\n",
    "# This function detects genre imbalance by comparing the most frequent\n",
    "# genre (major) to the least frequent genre (minor) in the dataset\n",
    "def major_minor_ratio_genres(movies_df):\n",
    "    # Create a copy of the dataframe to avoid modifying the original data\n",
    "    movies_df = movies_df.copy()\n",
    "    \n",
    "    # Split the genre string into a list of individual genres\n",
    "    movies_df[\"genres\"] = movies_df[\"genres\"].str.split(\"|\")\n",
    "    \n",
    "    # Count the occurrence of each genre across all movies\n",
    "    genre_counts = {}\n",
    "    for genre_list in movies_df[\"genres\"]:\n",
    "        if isinstance(genre_list, list):\n",
    "            for g in genre_list:\n",
    "                # Exclude placeholder entries that do not represent real genres\n",
    "                if g != \"(no genres listed)\":\n",
    "                    genre_counts[g] = genre_counts.get(g, 0) + 1\n",
    "    \n",
    "    # Convert the genre counts dictionary into a Pandas Series for easier analysis\n",
    "    genre_counts = pd.Series(genre_counts)\n",
    "    \n",
    "    # Identify the most common (major) and least common (minor) genres\n",
    "    major = genre_counts.max()\n",
    "    minor = genre_counts.min()\n",
    "    \n",
    "    # Compute the major‚Äìminor ratio as an indicator of genre imbalance\n",
    "    ratio = major / minor\n",
    "    \n",
    "    # Print detailed diagnostic information for bias analysis\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä BIAS DETECTION - Major-Minor Ratio\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Display the most frequent genres (top 10)\n",
    "    print(\"üé≠ Genre Counts (Top 10):\")\n",
    "    print(genre_counts.sort_values(ascending=False).head(10))\n",
    "    \n",
    "    # Display the least frequent genres (bottom 10)\n",
    "    print(\"\\nüìâ Genre Counts (Bottom 10):\")\n",
    "    print(genre_counts.sort_values().head(10))\n",
    "    \n",
    "    # Report the imbalance statistics\n",
    "    print(f\"\\nüü¶ Major‚ÄìMinor Ratio: {ratio:.2f}\")\n",
    "    print(f\"Most common genre has: {major} movies\")\n",
    "    print(f\"Least common genre has: {minor} movies\")\n",
    "    \n",
    "    # Interpret the imbalance severity using a simple threshold\n",
    "    if ratio > 10:\n",
    "        print(\"‚ö†Ô∏è Warning: Strong genre imbalance detected!\")\n",
    "    else:\n",
    "        print(\"‚úÖ Genre distribution is reasonably balanced.\")\n",
    "    \n",
    "    # Return the imbalance ratio and full genre count distribution\n",
    "    return ratio, genre_counts\n",
    "\n",
    "# Run the bias detection function on the movies dataset\n",
    "ratio, genre_counts = major_minor_ratio_genres(movies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c7b8308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "‚öñÔ∏è  APPLYING IDF-BASED REWEIGHTING\n",
      "============================================================\n",
      "\n",
      "Genre Weights (IDF):\n",
      "\n",
      "Top 5 Highest Weights (rare genres - boosted):\n",
      "   Film-Noir           : weight=4.715 (appears in 87 movies)\n",
      "   IMAX                : weight=4.118 (appears in 158 movies)\n",
      "   Western             : weight=4.063 (appears in 167 movies)\n",
      "   Musical             : weight=3.370 (appears in 334 movies)\n",
      "   War                 : weight=3.235 (appears in 382 movies)\n",
      "\n",
      "Top 5 Lowest Weights (common genres - reduced):\n",
      "   Romance             : weight=1.805 (appears in 1596 movies)\n",
      "   Action              : weight=1.670 (appears in 1828 movies)\n",
      "   Thriller            : weight=1.634 (appears in 1894 movies)\n",
      "   Comedy              : weight=0.950 (appears in 3756 movies)\n",
      "   Drama               : weight=0.800 (appears in 4361 movies)\n"
     ]
    }
   ],
   "source": [
    "# ===== 4) Calculate IDF Weights =====\n",
    "# This section applies IDF-based reweighting to reduce genre imbalance\n",
    "# by assigning higher weights to rare genres and lower weights to common ones\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚öñÔ∏è  APPLYING IDF-BASED REWEIGHTING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compute the total number of movies with valid genre information\n",
    "total_movies = len(movies[movies['genres'] != \"(no genres listed)\"])\n",
    "\n",
    "# Dictionary to store IDF weights for each genre\n",
    "genre_weights = {}\n",
    "\n",
    "# Calculate the IDF weight for each genre based on its frequency\n",
    "for genre, count in genre_counts.items():\n",
    "    # IDF formula: log(total_movies / number_of_movies_with_genre)\n",
    "    idf_weight = np.log(total_movies / count)\n",
    "    genre_weights[genre] = idf_weight\n",
    "\n",
    "# Display the computed IDF weights for interpretability\n",
    "print(\"\\nGenre Weights (IDF):\")\n",
    "\n",
    "# Sort genres by IDF weight in descending order\n",
    "sorted_weights = sorted(genre_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Show genres with the highest IDF weights (rare genres that are boosted)\n",
    "print(\"\\nTop 5 Highest Weights (rare genres - boosted):\")\n",
    "for genre, weight in sorted_weights[:5]:\n",
    "    count = genre_counts[genre]\n",
    "    print(f\"   {genre:20s}: weight={weight:.3f} (appears in {count} movies)\")\n",
    "\n",
    "# Show genres with the lowest IDF weights (common genres that are down-weighted)\n",
    "print(\"\\nTop 5 Lowest Weights (common genres - reduced):\")\n",
    "for genre, weight in sorted_weights[-5:]:\n",
    "    count = genre_counts[genre]\n",
    "    print(f\"   {genre:20s}: weight={weight:.3f} (appears in {count} movies)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7866cbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total unique genres: 19\n",
      "Top 50 tags: ['In Netflix queue', 'atmospheric', 'thought-provoking', 'superhero', 'funny', 'surreal', 'Disney', 'religion', 'sci-fi', 'quirky']...\n",
      "\n",
      "============================================================\n",
      "DATASET PREPARATION\n",
      "============================================================\n",
      "Total ratings: 100836\n",
      "Positive ratings (>= 4.0): 48580\n",
      "Unique users: 610\n",
      "Unique movies in ratings: 9724\n",
      "Movies common to both ratings and movies: 9724\n"
     ]
    }
   ],
   "source": [
    "# ===== 5) Prepare Content Features =====\n",
    "# Split the genre string into lists so each movie can have multiple genres\n",
    "movies['genres'] = movies['genres'].str.split('|')\n",
    "\n",
    "# Collect all unique genres across the dataset\n",
    "all_genres = set()\n",
    "for genre_list in movies['genres']:\n",
    "    if isinstance(genre_list, list):\n",
    "        for genre in genre_list:\n",
    "            # Exclude placeholder values that do not represent real genres\n",
    "            if genre != \"(no genres listed)\":\n",
    "                all_genres.add(genre)\n",
    "\n",
    "# Select the top 50 most frequent user-generated tags\n",
    "# These tags act as additional content features for the hybrid model\n",
    "top_tags = tags['tag'].value_counts().head(50).index.tolist()\n",
    "\n",
    "# Display summary statistics for content features\n",
    "print(f\"\\nTotal unique genres: {len(all_genres)}\")\n",
    "print(f\"Top 50 tags: {top_tags[:10]}...\")\n",
    "\n",
    "# ===== 6) Prepare Dataset =====\n",
    "# Define the threshold for converting explicit ratings into positive implicit feedback\n",
    "RATING_THRESHOLD = 4.0\n",
    "\n",
    "# Filter ratings to keep only positive interactions\n",
    "positive = ratings[ratings[\"rating\"] >= RATING_THRESHOLD].copy()\n",
    "\n",
    "# Display dataset preparation summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"DATASET PREPARATION\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total ratings: {len(ratings)}\")\n",
    "print(f\"Positive ratings (>= {RATING_THRESHOLD}): {len(positive)}\")\n",
    "\n",
    "# Extract unique users and movies from the ratings data\n",
    "all_users = ratings[\"userId\"].unique()\n",
    "all_items = ratings[\"movieId\"].unique()\n",
    "\n",
    "# Display dataset cardinality\n",
    "print(f\"Unique users: {len(all_users)}\")\n",
    "print(f\"Unique movies in ratings: {len(all_items)}\")\n",
    "\n",
    "# Identify movies that appear in both the ratings and movies metadata\n",
    "movies_in_ratings = movies[movies['movieId'].isin(all_items)]\n",
    "print(f\"Movies common to both ratings and movies: {len(movies_in_ratings)}\")\n",
    "\n",
    "# Create and initialize the LightFM Dataset object\n",
    "dataset = Dataset()\n",
    "\n",
    "# Fit the dataset with users, items, and combined content features\n",
    "# Content features include both genres and top user-generated tags\n",
    "dataset.fit(\n",
    "    users=all_users,\n",
    "    items=all_items,\n",
    "    item_features=list(all_genres) + top_tags\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36cabc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Built weighted features for 9724 movies\n"
     ]
    }
   ],
   "source": [
    "# ===== 7) Build Interactions with Weighted Features =====\n",
    "# This function builds the user‚Äìitem interaction matrix and applies\n",
    "# genre-based reweighting to item features to mitigate genre imbalance\n",
    "def prepare_features_with_reweighting(genre_weights, reweight_strength=1.0):\n",
    "    \"\"\"\n",
    "    Build item features with genre reweighting\n",
    "    \n",
    "    Args:\n",
    "        genre_weights: Dictionary containing IDF-based weights for each genre\n",
    "        reweight_strength: Controls how strongly the reweighting is applied\n",
    "                            (0 = no reweighting, 1 = full reweighting)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build the user‚Äìitem interaction matrix using positive implicit feedback\n",
    "    interactions, _ = dataset.build_interactions(\n",
    "        [(row.userId, row.movieId) for row in positive.itertuples(index=False)]\n",
    "    )\n",
    "    \n",
    "    # Prepare a list to store weighted item features for each movie\n",
    "    item_features_list = []\n",
    "    \n",
    "    # Iterate over all movies in the dataset\n",
    "    for movie_id in all_items:\n",
    "        # Retrieve the genre list associated with the current movie\n",
    "        movie_genres = movies[movies['movieId'] == movie_id]['genres']\n",
    "        \n",
    "        # Dictionary to store weighted genre features for the current movie\n",
    "        weighted_features = {}\n",
    "        \n",
    "        if len(movie_genres) > 0:\n",
    "            genres_str = movie_genres.iloc[0]\n",
    "            if isinstance(genres_str, list):\n",
    "                for genre in genres_str:\n",
    "                    # Ignore placeholder values that do not represent real genres\n",
    "                    if genre != \"(no genres listed)\":\n",
    "                        # Retrieve the base IDF weight for the genre\n",
    "                        base_weight = genre_weights.get(genre, 1.0)\n",
    "                        \n",
    "                        # Adjust the weight based on the chosen reweighting strength\n",
    "                        weight = 1.0 + (base_weight - 1.0) * reweight_strength\n",
    "                        \n",
    "                        # Store the weighted genre feature using the required dictionary format\n",
    "                        weighted_features[genre] = weight\n",
    "        \n",
    "        # Append the movie ID and its weighted features to the feature list\n",
    "        item_features_list.append((movie_id, weighted_features))\n",
    "    \n",
    "    # Report successful construction of weighted features\n",
    "    print(f\"‚úÖ Built weighted features for {len(item_features_list)} movies\")\n",
    "    \n",
    "    # Convert the weighted feature list into a sparse item feature matrix\n",
    "    item_features_matrix = dataset.build_item_features(item_features_list)\n",
    "    \n",
    "    # Return the interaction matrix and the weighted item feature matrix\n",
    "    return interactions, item_features_matrix\n",
    "\n",
    "# Build interactions and item features using controlled genre reweighting\n",
    "interactions, item_features = prepare_features_with_reweighting(\n",
    "    genre_weights, \n",
    "    reweight_strength=0.7  # Controls how strongly genre imbalance is corrected\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f068db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Final check:\n",
      "   all_items: 9724 movies\n",
      "   item_features: 9724 rows\n",
      "   Match: True\n",
      "‚úÖ READY FOR HYBRID RECOMMENDATIONS!\n"
     ]
    }
   ],
   "source": [
    "# ===== 8) Final check =====\n",
    "# Perform a final consistency check before training the hybrid recommendation model\n",
    "\n",
    "# Print a summary header for the final validation step\n",
    "print(f\"\\nüîç Final check:\")\n",
    "\n",
    "# Display the total number of unique movies used in the dataset\n",
    "print(f\"   all_items: {len(all_items)} movies\")\n",
    "\n",
    "# Display the number of rows in the item feature matrix\n",
    "# Each row should correspond to exactly one movie\n",
    "print(f\"   item_features: {item_features.shape[0]} rows\")\n",
    "\n",
    "# Verify that the number of movies matches the number of item feature rows\n",
    "print(f\"   Match: {len(all_items) == item_features.shape[0]}\")\n",
    "\n",
    "# If the counts match, the dataset is correctly prepared for hybrid modeling\n",
    "if len(all_items) == item_features.shape[0]:\n",
    "    print(\"‚úÖ READY FOR HYBRID RECOMMENDATIONS!\")\n",
    "else:\n",
    "    # If the counts do not match, there is a misalignment that must be fixed\n",
    "    print(\"‚ùå NEED TO FIX ITEM FEATURES!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e32e3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training data: 38864 interactions\n",
      "Testing data: 9716 interactions\n",
      "Testing data after filtering: 9142 interactions\n",
      "Train interactions shape: (610, 9724)\n",
      "Test interactions shape: (610, 9724)\n",
      "Item features shape: (9724, 9793)\n"
     ]
    }
   ],
   "source": [
    "# ===== 9) Split Data =====\n",
    "# Split the positive implicit interactions into training and testing sets\n",
    "# using a fixed random seed to ensure reproducibility\n",
    "train_df, test_df = train_test_split(\n",
    "    positive,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Display the number of interactions in each split\n",
    "print(f\"\\nTraining data: {len(train_df)} interactions\")\n",
    "print(f\"Testing data: {len(test_df)} interactions\")\n",
    "\n",
    "# Ensure that all users and items in the test set\n",
    "# also appear in the training set\n",
    "# This avoids cold-start issues during evaluation\n",
    "train_users = set(train_df[\"userId\"].unique())\n",
    "train_items = set(train_df[\"movieId\"].unique())\n",
    "\n",
    "# Filter the test set to keep only valid user‚Äìitem pairs\n",
    "test_df = test_df[\n",
    "    test_df[\"userId\"].isin(train_users) & \n",
    "    test_df[\"movieId\"].isin(train_items)\n",
    "].copy()\n",
    "\n",
    "# Display the size of the filtered test set\n",
    "print(f\"Testing data after filtering: {len(test_df)} interactions\")\n",
    "\n",
    "# Helper function to convert a dataframe of interactions\n",
    "# into a LightFM-compatible interaction matrix\n",
    "def prepare_interactions(df):\n",
    "    return dataset.build_interactions(\n",
    "        [(row.userId, row.movieId) for row in df.itertuples(index=False)]\n",
    "    )[0]\n",
    "\n",
    "# Build the final interaction matrices for training and testing\n",
    "train = prepare_interactions(train_df)\n",
    "test = prepare_interactions(test_df)\n",
    "\n",
    "# Display the shapes of the interaction matrices and item feature matrix\n",
    "print(f\"Train interactions shape: {train.shape}\")\n",
    "print(f\"Test interactions shape: {test.shape}\")\n",
    "print(f\"Item features shape: {item_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "badfd2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 10) Evaluation Function =====\n",
    "# This function evaluates the recommendation model using ranking-based metrics\n",
    "# on both training and testing interaction matrices\n",
    "def evaluate_model(model, train_interactions, test_interactions, item_features, k=10):\n",
    "    \n",
    "    # -------- Evaluation on training data --------\n",
    "    # Compute Precision@k on the training set\n",
    "    prec_tr = precision_at_k(\n",
    "        model, \n",
    "        train_interactions, \n",
    "        item_features=item_features, \n",
    "        k=k, \n",
    "        num_threads=1\n",
    "    ).mean()\n",
    "    \n",
    "    # Compute AUC on the training set to measure overall ranking quality\n",
    "    auc_tr = auc_score(\n",
    "        model, \n",
    "        train_interactions, \n",
    "        item_features=item_features, \n",
    "        num_threads=1\n",
    "    ).mean()\n",
    "    \n",
    "    # Compute Recall@k on the training set to measure retrieval completeness\n",
    "    rec_tr = recall_at_k(\n",
    "        model, \n",
    "        train_interactions, \n",
    "        item_features=item_features,\n",
    "        k=k, \n",
    "        num_threads=1\n",
    "    ).mean()   # Recall@k on training data\n",
    "\n",
    "    # -------- Evaluation on test data --------\n",
    "    # Compute Precision@k on the test set using the training interactions\n",
    "    # as the reference for known positives\n",
    "    prec_te = precision_at_k(\n",
    "        model, \n",
    "        test_interactions, \n",
    "        train_interactions=train_interactions,\n",
    "        item_features=item_features, \n",
    "        k=k, \n",
    "        num_threads=1\n",
    "    ).mean()\n",
    "    \n",
    "    # Compute AUC on the test set to assess generalization performance\n",
    "    auc_te = auc_score(\n",
    "        model, \n",
    "        test_interactions, \n",
    "        train_interactions=train_interactions,\n",
    "        item_features=item_features, \n",
    "        num_threads=1\n",
    "    ).mean()\n",
    "    \n",
    "    # Compute Recall@k on the test set\n",
    "    rec_te = recall_at_k(\n",
    "        model, \n",
    "        test_interactions, \n",
    "        train_interactions=train_interactions,\n",
    "        item_features=item_features,\n",
    "        k=k, \n",
    "        num_threads=1\n",
    "    ).mean()  # Recall@k on test data\n",
    "\n",
    "    # -------- Print evaluation results --------\n",
    "    # Display Precision@k for both training and testing sets\n",
    "    print(\n",
    "        f\"Precision@{k}: train {prec_tr:.4f} ({prec_tr*100:.2f}%), \"\n",
    "        f\"test {prec_te:.4f} ({prec_te*100:.2f}%)\"\n",
    "    )\n",
    "    \n",
    "    # Display Recall@k for both training and testing sets\n",
    "    print(\n",
    "        f\"Recall@{k}:    train {rec_tr:.4f} ({rec_tr*100:.2f}%), \"\n",
    "        f\"test {rec_te:.4f} ({rec_te*100:.2f}%)  (Recommendation Accuracy)\"\n",
    "    )\n",
    "    \n",
    "    # Display AUC for both training and testing sets\n",
    "    print(f\"AUC:           train {auc_tr:.4f}, test {auc_te:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2fee4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Hybrid Model Training (WARP + IDF-Weighted Genres)\n",
      "Checkpoint saved after each epoch\n",
      "==================================================\n",
      "\n",
      "Epoch 1/15\n",
      "üíæ Checkpoint saved\n",
      "\n",
      "Epoch 2/15\n",
      "üíæ Checkpoint saved\n",
      "\n",
      "Epoch 3/15\n",
      "üíæ Checkpoint saved\n",
      "\n",
      "Epoch 4/15\n",
      "üíæ Checkpoint saved\n",
      "\n",
      "Epoch 5/15\n",
      "üíæ Checkpoint saved\n",
      "\n",
      "Epoch 6/15\n",
      "üíæ Checkpoint saved\n",
      "\n",
      "Epoch 7/15\n",
      "üíæ Checkpoint saved\n",
      "\n",
      "Epoch 8/15\n",
      "üíæ Checkpoint saved\n",
      "\n",
      "Epoch 9/15\n",
      "üíæ Checkpoint saved\n",
      "\n",
      "Epoch 10/15\n",
      "üíæ Checkpoint saved\n",
      "\n",
      "Epoch 11/15\n",
      "üíæ Checkpoint saved\n",
      "\n",
      "Epoch 12/15\n",
      "üíæ Checkpoint saved\n",
      "\n",
      "Epoch 13/15\n",
      "üíæ Checkpoint saved\n",
      "\n",
      "Epoch 14/15\n",
      "üíæ Checkpoint saved\n",
      "\n",
      "Epoch 15/15\n",
      "üíæ Checkpoint saved\n",
      "Precision@10: train 0.2993 (29.93%), test 0.1173 (11.73%)\n",
      "Recall@10:    train 0.0828 (8.28%), test 0.1000 (10.00%)  (Recommendation Accuracy)\n",
      "AUC:           train 0.9486, test 0.9305\n"
     ]
    }
   ],
   "source": [
    "# ===== 11) Train Hybrid Model with Epoch-level Checkpointing =====\n",
    "# This section trains the hybrid LightFM model using epoch-level training\n",
    "# and saves a checkpoint after each epoch to ensure recoverability\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Hybrid Model Training (WARP + IDF-Weighted Genres)\")\n",
    "print(\"Checkpoint saved after each epoch\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "import pickle\n",
    "from lightfm import LightFM\n",
    "\n",
    "# Define the total number of training epochs\n",
    "num_epochs = 15\n",
    "\n",
    "# Initialize the LightFM hybrid model with WARP loss\n",
    "# WARP is chosen to optimize ranking quality in top-N recommendations\n",
    "model_hybrid = LightFM(\n",
    "    loss=\"warp\",\n",
    "    learning_rate=0.05,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model incrementally, one epoch at a time\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    # Perform one epoch of training using user‚Äìitem interactions\n",
    "    # and IDF-weighted item features\n",
    "    model_hybrid.fit_partial(\n",
    "        train,\n",
    "        item_features=item_features,\n",
    "        epochs=1,\n",
    "        num_threads=1\n",
    "    )\n",
    "\n",
    "    # Save a model checkpoint after each epoch\n",
    "    # This allows recovery in case of interruption\n",
    "    with open(\"lightfm_hybrid_checkpoint.pkl\", \"wb\") as f:\n",
    "        pickle.dump(model_hybrid, f)\n",
    "\n",
    "    print(\"üíæ Checkpoint saved\")\n",
    "\n",
    "# ===== 11a) Final Evaluation =====\n",
    "# Evaluate the final trained model on both training and testing data\n",
    "# using ranking-based metrics (Precision@k, Recall@k, and AUC)\n",
    "evaluate_model(model_hybrid, train, test, item_features, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65a84dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üéØ HYBRID RECOMMENDATIONS (IDF-Weighted)\n",
      "============================================================\n",
      "üìä Hybrid predictions for 9724 movies\n",
      "\n",
      "üîç Generating recommendations for user 1...\n",
      "üé¨ User 1 - Top 5 Hybrid Recommendations:\n",
      "   1. Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981)\n",
      "      ‚≠ê Score: -0.730 | üé≠ ['Action', 'Adventure']\n",
      "   2. Shawshank Redemption, The (1994)\n",
      "      ‚≠ê Score: -0.825 | üé≠ ['Crime', 'Drama']\n",
      "   3. Matrix, The (1999)\n",
      "      ‚≠ê Score: -0.846 | üé≠ ['Action', 'Sci-Fi', 'Thriller']\n",
      "   4. Terminator 2: Judgment Day (1991)\n",
      "      ‚≠ê Score: -0.850 | üé≠ ['Action', 'Sci-Fi']\n",
      "   5. Star Wars: Episode IV - A New Hope (1977)\n",
      "      ‚≠ê Score: -0.874 | üé≠ ['Action', 'Adventure', 'Sci-Fi']\n",
      "‚è± Response time for user 1: 2.455 seconds\n",
      "\n",
      "üîç Generating recommendations for user 2...\n",
      "üé¨ User 2 - Top 5 Hybrid Recommendations:\n",
      "   1. Silence of the Lambs, The (1991)\n",
      "      ‚≠ê Score: 1.579 | üé≠ ['Crime', 'Horror', 'Thriller']\n",
      "   2. Shining, The (1980)\n",
      "      ‚≠ê Score: 1.522 | üé≠ ['Horror']\n",
      "   3. Pulp Fiction (1994)\n",
      "      ‚≠ê Score: 1.475 | üé≠ ['Comedy', 'Crime', 'Drama', 'Thriller']\n",
      "   4. Shawshank Redemption, The (1994)\n",
      "      ‚≠ê Score: 1.445 | üé≠ ['Crime', 'Drama']\n",
      "   5. Godfather, The (1972)\n",
      "      ‚≠ê Score: 1.312 | üé≠ ['Crime', 'Drama']\n",
      "‚è± Response time for user 2: 2.306 seconds\n",
      "\n",
      "üîç Generating recommendations for user 3...\n",
      "üé¨ User 3 - Top 5 Hybrid Recommendations:\n",
      "   1. Alien (1979)\n",
      "      ‚≠ê Score: 1.301 | üé≠ ['Horror', 'Sci-Fi']\n",
      "   2. Terminator 2: Judgment Day (1991)\n",
      "      ‚≠ê Score: 1.291 | üé≠ ['Action', 'Sci-Fi']\n",
      "   3. Matrix, The (1999)\n",
      "      ‚≠ê Score: 1.248 | üé≠ ['Action', 'Sci-Fi', 'Thriller']\n",
      "   4. Shining, The (1980)\n",
      "      ‚≠ê Score: 1.177 | üé≠ ['Horror']\n",
      "   5. Star Wars: Episode V - The Empire Strikes Back (1980)\n",
      "      ‚≠ê Score: 1.160 | üé≠ ['Action', 'Adventure', 'Sci-Fi']\n",
      "‚è± Response time for user 3: 2.392 seconds\n"
     ]
    }
   ],
   "source": [
    "# ===== 12) Generate Sample Recommendations =====\n",
    "# This function generates sample movie recommendations for selected users\n",
    "# using the trained hybrid LightFM model and proper ID mappings\n",
    "def sample_recommendations(model, user_ids, item_features, dataset, n_items=5):\n",
    "    \"\"\"Generate movie recommendations using hybrid model with proper ID mapping\"\"\"\n",
    "    \n",
    "    # Retrieve internal-to-original ID mappings from the LightFM dataset\n",
    "    user_id_map, user_feature_map, item_id_map, item_feature_map = dataset.mapping()\n",
    "    \n",
    "    # Get the list of all available internal movie IDs\n",
    "    available_movies = list(item_id_map.values())\n",
    "    print(f\"üìä Hybrid predictions for {len(available_movies)} movies\")\n",
    "    \n",
    "    # Generate recommendations for each specified user\n",
    "    for user_id in user_ids:\n",
    "        print(f\"\\nüîç Generating recommendations for user {user_id}...\")\n",
    "        user_start = time.time()\n",
    "        \n",
    "        # Convert external user ID to internal LightFM user ID\n",
    "        user_internal_id = user_id_map.get(user_id)\n",
    "        if user_internal_id is None:\n",
    "            print(f\"‚ùå User {user_id} not found in dataset\")\n",
    "            continue\n",
    "        \n",
    "        scores = []\n",
    "        original_movie_ids = []\n",
    "        \n",
    "        # Predict scores for all movies for the given user\n",
    "        for movie_internal_id in available_movies:\n",
    "            score = model.predict(\n",
    "                np.array([user_internal_id], dtype=np.int32), \n",
    "                np.array([movie_internal_id], dtype=np.int32),\n",
    "                item_features=item_features,\n",
    "                num_threads=1\n",
    "            )[0]\n",
    "            scores.append(score)\n",
    "            \n",
    "            # Map internal movie ID back to the original movieId\n",
    "            original_id = [k for k, v in item_id_map.items() if v == movie_internal_id][0]\n",
    "            original_movie_ids.append(original_id)\n",
    "        \n",
    "        # Convert scores list to NumPy array for sorting\n",
    "        scores = np.array(scores)\n",
    "        \n",
    "        # Select the indices of the top-N highest scoring movies\n",
    "        top_indices = np.argsort(-scores)[:n_items]\n",
    "        top_movies = []\n",
    "        \n",
    "        # Retrieve metadata (title and genres) for the top recommendations\n",
    "        for idx in top_indices:\n",
    "            original_movie_id = original_movie_ids[idx]\n",
    "            movie_data = movies[movies['movieId'] == original_movie_id]\n",
    "            if len(movie_data) > 0:\n",
    "                title = movie_data['title'].values[0]\n",
    "                genres = movie_data['genres'].values[0]\n",
    "                top_movies.append((title, genres, scores[idx]))\n",
    "        \n",
    "        # Display the top-N recommendations for the current user\n",
    "        print(f\"üé¨ User {user_id} - Top {n_items} Hybrid Recommendations:\")\n",
    "        for i, (title, genres, score) in enumerate(top_movies, 1):\n",
    "            print(f\"   {i}. {title}\")\n",
    "            print(f\"      ‚≠ê Score: {score:.3f} | üé≠ {genres}\")\n",
    "        \n",
    "        # Measure and display the response time for generating recommendations\n",
    "        user_elapsed = time.time() - user_start\n",
    "        print(f\"‚è± Response time for user {user_id}: {user_elapsed:.3f} seconds\")\n",
    "\n",
    "\n",
    "# Display sample recommendations for the first three users\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ HYBRID RECOMMENDATIONS (IDF-Weighted)\")\n",
    "print(\"=\"*60)\n",
    "sample_users = list(all_users)[:3]\n",
    "sample_recommendations(model_hybrid, sample_users, item_features, dataset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
